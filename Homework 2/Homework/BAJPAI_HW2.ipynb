{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b2775d",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING & DATA MINING: BAJPAI_HW2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531a1cf",
   "metadata": {},
   "source": [
    "\n",
    "# [C] Problem 2:  Linear regression (15 points) \n",
    "\n",
    "In this problem, you will use an existing package of your choice for training and testing a linear regression model for the house prediction\n",
    "dataset.\n",
    "\n",
    "1. Use an existing package to train a multiple linear regression model on the training set using all the features (except the ones excluded above). Report the coefficients of the linear regression models and the following metrics on the training data: (1) MSE metric; (2) $R^2$ metric.\n",
    "2. Evaluate the model on the testing set. Report the MSE and $R^2$ metrics on the testing set.\n",
    "3. Interpret the results in your own words. Which features contribute mostly to the linear regression model? Is the model fitting the data well? How large is the model error? How do the training and testing MSE relate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc0409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:25:57.497175Z",
     "iopub.status.busy": "2026-02-13T15:25:57.496746Z",
     "iopub.status.idle": "2026-02-13T15:26:02.375356Z",
     "shell.execute_reply": "2026-02-13T15:26:02.374042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1000, 19)\n",
      "Test shape: (1000, 21)\n",
      "\n",
      "Train columns: ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
      "\n",
      "Test columns: ['id', 'date', 'price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
       "1  221900.0         3       1.00         1180      5650     1.0           0   \n",
       "2  538000.0         3       2.25         2570      7242     2.0           0   \n",
       "3  180000.0         2       1.00          770     10000     1.0           0   \n",
       "4  604000.0         4       3.00         1960      5000     1.0           0   \n",
       "5  510000.0         3       2.00         1680      8080     1.0           0   \n",
       "\n",
       "   view  condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  \\\n",
       "1     0          3      7        1180              0      1955             0   \n",
       "2     0          3      7        2170            400      1951          1991   \n",
       "3     0          3      6         770              0      1933             0   \n",
       "4     0          5      7        1050            910      1965             0   \n",
       "5     0          3      8        1680              0      1987             0   \n",
       "\n",
       "   zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "1    98178  47.5112 -122.257           1340        5650  \n",
       "2    98125  47.7210 -122.319           1690        7639  \n",
       "3    98028  47.7379 -122.233           2720        8062  \n",
       "4    98136  47.5208 -122.393           1360        5000  \n",
       "5    98074  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "train_df = pd.read_csv('../DataSets/train.csv', index_col=0)\n",
    "test_df = pd.read_csv('../DataSets/test.csv', index_col=0)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nTrain columns:\", list(train_df.columns))\n",
    "print(\"\\nTest columns:\", list(test_df.columns))\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01ff2baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.380841Z",
     "iopub.status.busy": "2026-02-13T15:26:02.380345Z",
     "iopub.status.idle": "2026-02-13T15:26:02.390138Z",
     "shell.execute_reply": "2026-02-13T15:26:02.388464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used (18): ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
      "X_train shape: (1000, 18), y_train shape: (1000,)\n",
      "X_test shape: (1000, 18), y_test shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_cols = [c for c in train_df.columns if c != 'price']\n",
    "\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df['price'].values\n",
    "\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['price'].values\n",
    "\n",
    "print(f\"Features used ({len(feature_cols)}): {feature_cols}\")\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8262fcf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.393375Z",
     "iopub.status.busy": "2026-02-13T15:26:02.393046Z",
     "iopub.status.idle": "2026-02-13T15:26:02.402066Z",
     "shell.execute_reply": "2026-02-13T15:26:02.401089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Coefficients (sklearn)\n",
      "Intercept (θ₀): 12,875,953.32\n",
      "\n",
      "  bedrooms            :    -16,666.9522\n",
      "  bathrooms           :     25,607.7942\n",
      "  sqft_living         :         82.4546\n",
      "  sqft_lot            :          0.3658\n",
      "  floors              :     21,086.6272\n",
      "  waterfront          :    714,809.8616\n",
      "  view                :     64,888.7151\n",
      "  condition           :     16,397.6327\n",
      "  grade               :     80,108.9815\n",
      "  sqft_above          :         42.4282\n",
      "  sqft_basement       :         40.0264\n",
      "  yr_built            :     -2,584.5539\n",
      "  yr_renovated        :         42.7528\n",
      "  zipcode             :       -462.7944\n",
      "  lat                 :    584,409.3321\n",
      "  long                :    -75,589.7285\n",
      "  sqft_living15       :         62.0428\n",
      "  sqft_lot15          :         -0.4467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Linear Regression Coefficients (sklearn)\")\n",
    "print(f\"Intercept (θ₀): {lr.intercept_:,.2f}\\n\")\n",
    "for name, coef in zip(feature_cols, lr.coef_):\n",
    "    print(f\"  {name:20s}: {coef:>15,.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98e4b860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.406067Z",
     "iopub.status.busy": "2026-02-13T15:26:02.405793Z",
     "iopub.status.idle": "2026-02-13T15:26:02.416260Z",
     "shell.execute_reply": "2026-02-13T15:26:02.414757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics\n",
      "MSE:  31,119,892,883.73\n",
      "R²:   0.729715\n",
      "\n",
      "Testing Metrics\n",
      "MSE:  57,161,532,843.15\n",
      "R²:   0.657155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train_pred = lr.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training Metrics\")\n",
    "print(f\"MSE:  {train_mse:,.2f}\")\n",
    "print(f\"R²:   {train_r2:.6f}\")\n",
    "print(f\"\\nTesting Metrics\")\n",
    "print(f\"MSE:  {test_mse:,.2f}\")\n",
    "print(f\"R²:   {test_r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74de14",
   "metadata": {},
   "source": [
    "\n",
    "# [C] Problem 3:  Implementing closed-form solution for linear regression (15 points) \n",
    "\n",
    "In this problem, you will implement your own linear regression model, using the closed-form solution we derived in class. You will also\n",
    "compare your model with the one trained with the package in Problem 2 on the same house price prediction dataset.\n",
    "\n",
    "- Implement the closed-from solution for multiple linear regression using matrix operations and train a model on the training set. Write\n",
    "a function to predict the response on a new testing point.\n",
    "- Compare the models given by your implementation with those trained in Problem 2 by the Python packages. Report the MSE and $R^2$\n",
    "metrics for the models you implemented on both training and testing sets and compare these metrics to the ones given by the package\n",
    "implementation from Problem 2. Discuss if the results of your implementation are similar to those of the package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d875bf73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.419776Z",
     "iopub.status.busy": "2026-02-13T15:26:02.419460Z",
     "iopub.status.idle": "2026-02-13T15:26:02.428104Z",
     "shell.execute_reply": "2026-02-13T15:26:02.426641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-Form Coefficients\n",
      "Intercept (θ₀): -69.33\n",
      "\n",
      "  bedrooms            :    -16,000.4205\n",
      "  bathrooms           :     25,536.1864\n",
      "  sqft_living         :         82.3904\n",
      "  sqft_lot            :          0.3725\n",
      "  floors              :     18,429.9517\n",
      "  waterfront          :    713,429.7818\n",
      "  view                :     64,062.7315\n",
      "  condition           :     17,366.1792\n",
      "  grade               :     79,002.0352\n",
      "  sqft_above          :         43.2269\n",
      "  sqft_basement       :         39.1586\n",
      "  yr_built            :     -2,458.3948\n",
      "  yr_renovated        :         43.7160\n",
      "  zipcode             :       -344.5240\n",
      "  lat                 :    583,625.3026\n",
      "  long                :    -84,306.5225\n",
      "  sqft_living15       :         65.1404\n",
      "  sqft_lot15          :         -0.4445\n"
     ]
    }
   ],
   "source": [
    "def closed_form_fit(X, y):\n",
    "    \"\"\"\n",
    "    Closed-form solution: θ = (X^T X)^{-1} X^T y\n",
    "    Automatically adds a bias column of ones.\n",
    "    Uses pseudoinverse for numerical stability.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    X_b = np.hstack([np.ones((N, 1)), X])\n",
    "   \n",
    "    theta = np.linalg.pinv(X_b.T @ X_b) @ X_b.T @ y\n",
    "    return theta\n",
    "\n",
    "def closed_form_predict(X, theta):\n",
    "    \"\"\"Predict using θ. Adds bias column automatically.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    X_b = np.hstack([np.ones((N, 1)), X])\n",
    "    return X_b @ theta\n",
    "\n",
    "\n",
    "theta_cf = closed_form_fit(X_train, y_train)\n",
    "\n",
    "print(\"Closed-Form Coefficients\")\n",
    "print(f\"Intercept (θ₀): {theta_cf[0]:,.2f}\\n\")\n",
    "for name, coef in zip(feature_cols, theta_cf[1:]):\n",
    "    print(f\"  {name:20s}: {coef:>15,.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5668dc09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.432619Z",
     "iopub.status.busy": "2026-02-13T15:26:02.432289Z",
     "iopub.status.idle": "2026-02-13T15:26:02.443663Z",
     "shell.execute_reply": "2026-02-13T15:26:02.441718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-Form vs sklearn Comparison\n",
      "Metric                       sklearn     Closed-Form     Match?\n",
      "Train MSE            31,119,892,883.73 31,153,404,796.07      FALSE\n",
      "Train R²                    0.729715        0.729424       TRUE\n",
      "Test MSE             57,161,532,843.15 57,184,543,400.15       TRUE\n",
      "Test R²                     0.657155        0.657017       TRUE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_train_pred_cf = closed_form_predict(X_train, theta_cf)\n",
    "y_test_pred_cf = closed_form_predict(X_test, theta_cf)\n",
    "\n",
    "cf_train_mse = mean_squared_error(y_train, y_train_pred_cf)\n",
    "cf_train_r2 = r2_score(y_train, y_train_pred_cf)\n",
    "cf_test_mse = mean_squared_error(y_test, y_test_pred_cf)\n",
    "cf_test_r2 = r2_score(y_test, y_test_pred_cf)\n",
    "\n",
    "print(\"Closed-Form vs sklearn Comparison\")\n",
    "print(f\"{'Metric':<20} {'sklearn':>15} {'Closed-Form':>15} {'Match?':>10}\")\n",
    "\n",
    "print(f\"{'Train MSE':<20} {train_mse:>15,.2f} {cf_train_mse:>15,.2f} {'TRUE' if abs(train_mse - cf_train_mse)/train_mse < 0.001 else 'FALSE':>10}\")\n",
    "print(f\"{'Train R²':<20} {train_r2:>15.6f} {cf_train_r2:>15.6f} {'TRUE' if abs(train_r2 - cf_train_r2) < 0.001 else 'FALSE':>10}\")\n",
    "print(f\"{'Test MSE':<20} {test_mse:>15,.2f} {cf_test_mse:>15,.2f} {'TRUE' if abs(test_mse - cf_test_mse)/test_mse < 0.001 else 'FALSE':>10}\")\n",
    "print(f\"{'Test R²':<20} {test_r2:>15.6f} {cf_test_r2:>15.6f} {'TRUE' if abs(test_r2 - cf_test_r2) < 0.001 else 'FALSE':>10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737ac08",
   "metadata": {},
   "source": [
    "\n",
    "# [C] Problem 4: Polynomial Regression (15 points) \n",
    "\n",
    "- Consider a feature $X$, a response variable $Y$, and $N$ samples of training data. Implement a polynomial regression model that fits a polynomial of degree $p$ to the data using the least-square method. Use your own implementation from Problem 3 and adapt it for polynomial\n",
    "regression. If $p=2$, the model will use two features ($X$ and $X^2$), if $p=3$ the model will use 3 features ($X,X^2,X^3$), and so on for larger values of $p$.\n",
    "- Consider the house price prediction problem with feature $X=$ `sqft_living`. Train a polynomial regression model for different values of $p \\le 5$ using your implementation. Include a table with the MSE and $R^2$ metrics on both the training and testing data for at least 3 different values of $p$. Discuss your observations on how the MSE and $R^2$ metrics change with the degree of the polynomial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a6cc8578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.447262Z",
     "iopub.status.busy": "2026-02-13T15:26:02.446901Z",
     "iopub.status.idle": "2026-02-13T15:26:02.465382Z",
     "shell.execute_reply": "2026-02-13T15:26:02.463975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression Results (Feature: sqft_living, standardized)\n",
      "\n",
      "  p=1:  Train MSE= 57,947,526,161.29  Train R²=0.4967  Test MSE= 88,575,978,543.10  Test R²=0.4687\n",
      "  p=2:  Train MSE= 54,822,665,116.28  Train R²=0.5238  Test MSE= 71,791,679,478.90  Test R²=0.5694\n",
      "  p=3:  Train MSE= 53,785,194,716.49  Train R²=0.5329  Test MSE= 99,833,483,762.81  Test R²=0.4012\n",
      "  p=4:  Train MSE= 52,795,774,757.64  Train R²=0.5415  Test MSE=250,979,274,284.93  Test R²=-0.5053\n",
      "  p=5:  Train MSE= 52,626,111,954.56  Train R²=0.5429  Test MSE=570,616,914,819.69  Test R²=-2.4225\n"
     ]
    }
   ],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    \"\"\"Create polynomial features [X, X^2, ..., X^degree] from a 1D array.\"\"\"\n",
    "    X = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "    features = np.hstack([X**d for d in range(1, degree + 1)])\n",
    "    return features\n",
    "\n",
    "\n",
    "sqft_idx = feature_cols.index('sqft_living')\n",
    "X_train_sqft_raw = X_train[:, sqft_idx]\n",
    "X_test_sqft_raw = X_test[:, sqft_idx]\n",
    "\n",
    "\n",
    "sqft_mean = X_train_sqft_raw.mean()\n",
    "sqft_std = X_train_sqft_raw.std()\n",
    "X_train_sqft = (X_train_sqft_raw - sqft_mean) / sqft_std\n",
    "X_test_sqft = (X_test_sqft_raw - sqft_mean) / sqft_std\n",
    "\n",
    "results = []\n",
    "for p in range(1, 6):\n",
    "    X_train_poly = polynomial_features(X_train_sqft, p)\n",
    "    X_test_poly = polynomial_features(X_test_sqft, p)\n",
    "    \n",
    "    theta_poly = closed_form_fit(X_train_poly, y_train)\n",
    "    \n",
    "    y_train_pred_p = closed_form_predict(X_train_poly, theta_poly)\n",
    "    y_test_pred_p = closed_form_predict(X_test_poly, theta_poly)\n",
    "    \n",
    "    tr_mse = mean_squared_error(y_train, y_train_pred_p)\n",
    "    tr_r2 = r2_score(y_train, y_train_pred_p)\n",
    "    te_mse = mean_squared_error(y_test, y_test_pred_p)\n",
    "    te_r2 = r2_score(y_test, y_test_pred_p)\n",
    "    \n",
    "    results.append({\n",
    "        'Degree p': p,\n",
    "        'Train MSE': tr_mse,\n",
    "        'Train R²': tr_r2,\n",
    "        'Test MSE': te_mse,\n",
    "        'Test R²': te_r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Polynomial Regression Results (Feature: sqft_living, standardized)\\n\")\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"  p={int(row['Degree p'])}:  Train MSE={row['Train MSE']:>18,.2f}  Train R²={row['Train R²']:.4f}  \"\n",
    "          f\"Test MSE={row['Test MSE']:>18,.2f}  Test R²={row['Test R²']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652aa1f3",
   "metadata": {},
   "source": [
    "\n",
    "# [C] Problem 5:  Gradient descent (20 points) \n",
    "\n",
    "In this problem, you will implement your own gradient descent algorithm and apply it to linear regression on the same house prediction dataset.\n",
    "\n",
    "1. Write code for gradient descent for training linear regression using the algorithm from class.\n",
    "2. Vary the value of the learning rate (at least 3 different values $\\alpha \\in \\{0.01,0.1,0.5\\}$) and report the value of the model parameter $\\theta$ after different number of iterations (10, 50, and 100). Include in a table the MSE and $R^2$ metrics on the training and testing set for the different number of iterations and different learning rates. You can choose more values of the learning rates to observe how the  behavior of the algorithm changes.\n",
    "3. Write some observations about the behavior of the algorithm: How do the metrics change with different learning rates; How many iterations are needed; Does the algorithm converge to the optimal solution, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d129c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.468716Z",
     "iopub.status.busy": "2026-02-13T15:26:02.468160Z",
     "iopub.status.idle": "2026-02-13T15:26:02.478060Z",
     "shell.execute_reply": "2026-02-13T15:26:02.475969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data standardized. Ready for gradient descent.\n",
      "X_train_gd shape: (1000, 19)\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(X, y, alpha, n_iterations):\n",
    "    \"\"\"\n",
    "    Gradient descent for linear regression.\n",
    "    X should already include bias column.\n",
    "    Returns: theta, history of MSE per iteration\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "    mse_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (2 / N) * (X.T @ errors)\n",
    "        theta = theta - alpha * gradient\n",
    "        mse = np.mean(errors**2)\n",
    "        mse_history.append(mse)\n",
    "    \n",
    "    return theta, mse_history\n",
    "\n",
    "\n",
    "train_mean = X_train.mean(axis=0)\n",
    "train_std = X_train.std(axis=0)\n",
    "train_std[train_std == 0] = 1\n",
    "\n",
    "X_train_std = (X_train - train_mean) / train_std\n",
    "X_test_std = (X_test - train_mean) / train_std  \n",
    "\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()\n",
    "y_train_std = (y_train - y_mean) / y_std\n",
    "\n",
    "X_train_gd = np.hstack([np.ones((X_train_std.shape[0], 1)), X_train_std])\n",
    "X_test_gd = np.hstack([np.ones((X_test_std.shape[0], 1)), X_test_std])\n",
    "\n",
    "print(\"Data standardized. Ready for gradient descent.\")\n",
    "print(f\"X_train_gd shape: {X_train_gd.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7832d5b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:02.482226Z",
     "iopub.status.busy": "2026-02-13T15:26:02.481871Z",
     "iopub.status.idle": "2026-02-13T15:26:02.508469Z",
     "shell.execute_reply": "2026-02-13T15:26:02.506382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: MSE and R² for Different α and Iterations\n",
      "\n",
      "     α  Iters          Train MSE   Train R²           Test MSE    Test R²\n",
      "  0.01     10  54,937,695,597.20     0.5229  90,984,069,004.80     0.4543\n",
      "  0.01     50  33,835,494,498.01     0.7061  61,554,159,699.20     0.6308\n",
      "  0.01    100  31,917,675,289.83     0.7228  58,826,933,975.95     0.6472\n",
      "\n",
      "   0.1     10  31,830,914,544.62     0.7235  58,700,970,746.93     0.6479\n",
      "   0.1     50  31,132,474,248.24     0.7296  57,267,194,457.13     0.6565\n",
      "   0.1    100  31,120,170,242.70     0.7297  57,173,036,650.63     0.6571\n",
      "\n",
      "   0.5     10 252,354,611,314,228,472,053,760.00 -2191773543198.5762 279,056,774,020,273,950,162,944.00 -1673735271297.2864\n",
      "   0.5     50 254,620,462,395,337,678,654,242,510,961,174,057,256,534,183,758,663,645,542,553,670,655,273,861,120.00 -2211453121973833116565324544295090216138419331286245199986032640.0000 281,562,341,120,199,251,826,498,820,251,636,008,664,761,944,574,359,796,752,342,428,813,705,084,928.00 -1688763238436775661382081260103363787246955691359497872304242688.0000\n",
      "   0.5    100 1,447,924,358,307,134,435,862,334,772,174,754,686,864,041,222,614,752,709,148,217,790,772,811,759,259,923,969,783,363,077,252,472,353,373,138,337,958,155,468,878,863,442,899,281,903,616.00 -12575646169350852963983258530322652766324241061679353775970155361495735195458918265403657258199842638870843635153804719592308736.0000 1,601,132,007,438,314,898,286,188,027,705,494,627,960,410,692,753,833,032,473,582,444,558,013,348,478,638,366,553,310,715,616,405,948,152,319,898,582,855,396,211,294,751,656,960,327,680.00 -9603318623110867320837864129162978148611403669617115551966214675500565646655460927497533580222603471569198005408355359117017088.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "\n",
    "print(\"Gradient Descent: MSE and R² for Different α and Iterations\\n\")\n",
    "print(f\"{'α':>6} {'Iters':>6} {'Train MSE':>18} {'Train R²':>10} {'Test MSE':>18} {'Test R²':>10}\")\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    for n_iter in iterations_list:\n",
    "        theta_gd, mse_hist = gradient_descent(X_train_gd, y_train_std, alpha, n_iter)\n",
    "        \n",
    "      \n",
    "        y_train_pred_gd = (X_train_gd @ theta_gd) * y_std + y_mean\n",
    "        y_test_pred_gd = (X_test_gd @ theta_gd) * y_std + y_mean\n",
    "        \n",
    "        tr_mse = mean_squared_error(y_train, y_train_pred_gd)\n",
    "        tr_r2 = r2_score(y_train, y_train_pred_gd)\n",
    "        te_mse = mean_squared_error(y_test, y_test_pred_gd)\n",
    "        te_r2 = r2_score(y_test, y_test_pred_gd)\n",
    "        \n",
    "        print(f\"{alpha:>6} {n_iter:>6} {tr_mse:>18,.2f} {tr_r2:>10.4f} {te_mse:>18,.2f} {te_r2:>10.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089bcea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:03.745170Z",
     "iopub.status.busy": "2026-02-13T15:26:03.744577Z",
     "iopub.status.idle": "2026-02-13T15:26:03.790754Z",
     "shell.execute_reply": "2026-02-13T15:26:03.788468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged Gradient Descent (α=0.1, 2000 iters) vs sklearn\n",
      "\n",
      "Metric                     sklearn     GD (converged)\n",
      "Train MSE        31,119,892,883.73  31,119,892,883.73\n",
      "Train R²                  0.729715           0.729715\n",
      "Test MSE         57,161,532,843.15  57,161,532,843.16\n",
      "Test R²                   0.657155           0.657155\n",
      "\n",
      "With enough iterations, GD converges to the sklearn solution.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theta_gd_final, _ = gradient_descent(X_train_gd, y_train_std, 0.1, 2000)\n",
    "\n",
    "y_train_pred_final = (X_train_gd @ theta_gd_final) * y_std + y_mean\n",
    "y_test_pred_final = (X_test_gd @ theta_gd_final) * y_std + y_mean\n",
    "\n",
    "gd_train_mse = mean_squared_error(y_train, y_train_pred_final)\n",
    "gd_test_mse = mean_squared_error(y_test, y_test_pred_final)\n",
    "gd_train_r2 = r2_score(y_train, y_train_pred_final)\n",
    "gd_test_r2 = r2_score(y_test, y_test_pred_final)\n",
    "\n",
    "print(\"Converged Gradient Descent (α=0.1, 2000 iters) vs sklearn\\n\")\n",
    "print(f\"{'Metric':<15} {'sklearn':>18} {'GD (converged)':>18}\")\n",
    "print(f\"{'Train MSE':<15} {train_mse:>18,.2f} {gd_train_mse:>18,.2f}\")\n",
    "print(f\"{'Train R²':<15} {train_r2:>18.6f} {gd_train_r2:>18.6f}\")\n",
    "print(f\"{'Test MSE':<15} {test_mse:>18,.2f} {gd_test_mse:>18,.2f}\")\n",
    "print(f\"{'Test R²':<15} {test_r2:>18.6f} {gd_test_r2:>18.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c44ec7",
   "metadata": {},
   "source": [
    "# [A/C] Problem 6: Ridge regularization (20 points)\n",
    "In this problem, you will derive the optimal parameters for ridge regression and train ridge regression models with different regularization levels. In ridge regression, the loss function includes a regularization term:\n",
    "\n",
    "$J(\\theta) = \\sum_{i=1}^N(h_{\\theta}(x_i)-y_i)^2 + \\lambda \\sum_{j=1}^d \\theta_j^2$\n",
    "\n",
    "1. **[A]** Write the derivation of the closed form solution for parameter $\\theta$ that minimizes the loss function $J(\\theta)$ in ridge regression.\n",
    "2. **[C]** Modify your implementation from Problem 5 to implement ridge regression with gradient descent.\n",
    "3. **[C]** Simulate $N=1000$ values of random variable $X_i$, distributed uniformly on interval $[-2,2]$. Simulate the values of random variable       $Y_i = 1 + 2X_i + e_i$, where $e_i$ is drawn from a Gaussian distribution $N(0, 2)$. Fit this data with linear regression, and also with ridge regression\n",
    "for different values of $\\lambda \\in \\{1,10,100,1000,10000\\}$. Print the slope, the MSE values, and the $R^2$ statistic for each case and write down some observations. What happens as the regularization parameter $\\lambda$ increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ab7537d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:03.794764Z",
     "iopub.status.busy": "2026-02-13T15:26:03.794256Z",
     "iopub.status.idle": "2026-02-13T15:26:03.802559Z",
     "shell.execute_reply": "2026-02-13T15:26:03.800833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge gradient descent function defined. ✅\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ridge_gradient_descent(X, y, alpha, n_iterations, lam):\n",
    "    \"\"\"\n",
    "    Gradient descent for ridge regression.\n",
    "    Cost: J(θ) = (1/N)||Xθ - y||² + λ||θ||²\n",
    "    Gradient: (2/N) X^T(Xθ - y) + 2λθ  (skip bias regularization)\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    theta = np.zeros(d)\n",
    "    mse_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        predictions = X @ theta\n",
    "        errors = predictions - y\n",
    "        gradient = (2 / N) * (X.T @ errors)\n",
    "        \n",
    "       \n",
    "        reg_gradient = 2 * lam * theta.copy()\n",
    "        reg_gradient[0] = 0\n",
    "        gradient += reg_gradient\n",
    "        \n",
    "        theta = theta - alpha * gradient\n",
    "        mse = np.mean(errors**2)\n",
    "        mse_history.append(mse)\n",
    "    \n",
    "    return theta, mse_history\n",
    "\n",
    "print(\"Ridge gradient descent function defined. ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36cb49ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T15:26:03.807167Z",
     "iopub.status.busy": "2026-02-13T15:26:03.806804Z",
     "iopub.status.idle": "2026-02-13T15:26:03.824423Z",
     "shell.execute_reply": "2026-02-13T15:26:03.822688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: intercept = 1.0, slope = 2.0\n",
      "OLS estimate:    intercept = 1.1948, slope = 1.9226\n",
      "\n",
      "    Method    Intercept      Slope        MSE         R²\n",
      "       OLS       1.1948     1.9226     3.8999     0.5639\n",
      "       λ=1       1.1947     1.9212     3.8999     0.5639\n",
      "      λ=10       1.1942     1.9086     3.9001     0.5639\n",
      "     λ=100       1.1897     1.7913     3.9234     0.5613\n",
      "    λ=1000       1.1631     1.1094     4.8021     0.4630\n",
      "   λ=10000       1.1288     0.2308     7.8044     0.1273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N_sim = 1000\n",
    "X_sim = np.random.uniform(-2, 2, N_sim)\n",
    "e_sim = np.random.normal(0, 2, N_sim) \n",
    "Y_sim = 1 + 2 * X_sim + e_sim\n",
    "\n",
    "X_sim_b = np.hstack([np.ones((N_sim, 1)), X_sim.reshape(-1, 1)])\n",
    "\n",
    "\n",
    "theta_ols = np.linalg.inv(X_sim_b.T @ X_sim_b) @ X_sim_b.T @ Y_sim\n",
    "\n",
    "print(\"True parameters: intercept = 1.0, slope = 2.0\")\n",
    "print(f\"OLS estimate:    intercept = {theta_ols[0]:.4f}, slope = {theta_ols[1]:.4f}\\n\")\n",
    "\n",
    "lambdas = [1, 10, 100, 1000, 10000]\n",
    "\n",
    "print(f\"{'Method':>10} {'Intercept':>12} {'Slope':>10} {'MSE':>10} {'R²':>10}\")\n",
    "\n",
    "\n",
    "\n",
    "y_pred_ols = X_sim_b @ theta_ols\n",
    "print(f\"{'OLS':>10} {theta_ols[0]:>12.4f} {theta_ols[1]:>10.4f} \"\n",
    "      f\"{mean_squared_error(Y_sim, y_pred_ols):>10.4f} {r2_score(Y_sim, y_pred_ols):>10.4f}\")\n",
    "\n",
    "for lam in lambdas:\n",
    "    I_reg = np.eye(X_sim_b.shape[1])\n",
    "    I_reg[0, 0] = 0  \n",
    "    theta_ridge = np.linalg.inv(X_sim_b.T @ X_sim_b + lam * I_reg) @ X_sim_b.T @ Y_sim\n",
    "    \n",
    "    y_pred_ridge = X_sim_b @ theta_ridge\n",
    "    ridge_mse = mean_squared_error(Y_sim, y_pred_ridge)\n",
    "    ridge_r2 = r2_score(Y_sim, y_pred_ridge)\n",
    "    \n",
    "    print(f\"{'λ=' + str(lam):>10} {theta_ridge[0]:>12.4f} {theta_ridge[1]:>10.4f} \"\n",
    "          f\"{ridge_mse:>10.4f} {ridge_r2:>10.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
